[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python & Data",
    "section": "",
    "text": "Welcome!\nPython is a powerful programming language that is suitable for scientific computing and general-purpose programming. Python is used to write scripts to work efficiently and reproducibly with scientific data.\nIn this workshop, we will utilize the Jupyter Notebook interface and take you from the basics of Python syntax to using the pandas package to work with data frames. In doing so, we aim to give you the tools and confidence to start exploring Python and all it has to offer.\nOur workshop material is licensed under a Creative Commons Attribution 4.0 International License. You can view the license on our GitHub repository."
  },
  {
    "objectID": "acknowledgements.html#contributors",
    "href": "acknowledgements.html#contributors",
    "title": "Acknowledgements",
    "section": "Contributors",
    "text": "Contributors\nThe following indivduals have contributed to the development of this workshop:\n\nJelle Treep\nChristine Staiger\nRoel Brouwer\nNeha Moopen\nStefano Rapisarda"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Time\nActivity\n\n\n\n\n9:00\nWalk-in, tech support\n\n\n9:30\nIntroductions\n\n\n10:00\nPython Basics: Exercises 1-6\n\n\n11:25\nRecap & Questions\n\n\n11:30\nCoffee break\n\n\n11:45\nProgramming: Exercises 7-9\n\n\n12:40\nRecap & Questions\n\n\n12:45\nLunch break\n\n\n13:30\nIntroduction to pandas & Importing data: Exercises 1-4\n\n\n14:15\nSubsetting and mutating data: Exercises 5-8\n\n\n14:55\nRecap & Questions\n\n\n15:00\nCoffee break\n\n\n15:15\nTransformations & tidy data: Exercises 9-12\n\n\n16:00\nData visualization: Exercises 13-15\n\n\n16:55\nFinal recap and closing"
  },
  {
    "objectID": "installation-and-setup.html#overview",
    "href": "installation-and-setup.html#overview",
    "title": "1  Installation & Setup",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nThe course materials are designed to be run on a personal computer. All of the software and data used are freely available online, and instructions on how to obtain them are provided below.\nIf you have any questions, or are not comfortable with doing the installation yourself join the RDM walk-in hours to ask for help."
  },
  {
    "objectID": "installation-and-setup.html#install-python",
    "href": "installation-and-setup.html#install-python",
    "title": "1  Installation & Setup",
    "section": "1.2 Install Python",
    "text": "1.2 Install Python\nIn this course, we will be using Python 3 with some of its most popular scientific libraries. Python is a popular language for research computing, and great for general-purpose programming as well. Although one can install a plain-vanilla Python and all required libraries by hand, we recommend installing Anaconda, a Python distribution that comes with everything we need for the lesson. Detailed installation instructions can be found in the Anaconda documentation, or by following the instructions below.\nRegardless of how you choose to install it, please make sure you install Python version 3.x (e.g., 3.9 is fine).\nWe will teach Python using the Jupyter Notebook, a programming environment that runs in a web browser (Jupyter Notebook will be installed by Anaconda). For this to work you will need a reasonably up-to-date browser. The current versions of the Chrome, Safari and Firefox browsers are all supported (some older browsers, including Internet Explorer version 9 and below, are not).\n\nWindowsMacOSLinux\n\n\n\nOpen https://www.anaconda.com/products/individual#download-section with your web browser.\nDownload the Anaconda for Windows installer with Python 3. (If you are not sure which version to choose, you probably want the 64-bit Graphical Installer Anaconda3-...-Windows-x86_64.exe)\nInstall Python 3 by running the Anaconda Installer, using all of the defaults for installation except make sure to check Add Anaconda to my PATH environment variable.\n\n\n\n\nOpen https://www.anaconda.com/products/individual#download-section with your web browser.\nDownload the Anaconda Installer with Python 3 for macOS (you can either use the Graphical or the Command Line Installer).\nInstall Python 3 by running the Anaconda Installer using all of the defaults for installation.\n\n\n\n\nOpen https://www.anaconda.com/products/individual#download-section with your web browser.\nDownload the Anaconda Installer with Python 3 for Linux.  (The installation requires using the shell. If you aren’t comfortable doing the installation yourself stop here and come to the walk-in hours to ask for help.)\nOpen a terminal window and navigate to the directory where the executable is downloaded (e.g., cd ~/Downloads).\nType\nbash Anaconda3-\nand then press Tab to autocomplete the full file name. The name of file you just downloaded should appear.\nPress Enter (or Return depending on your keyboard). You will follow the text-only prompts. To move through the text, press Spacebar. Type yes and press enter to approve the license. Press Enter (or Return) to approve the default location for the files. Type yes and press Enter (or Return) to prepend Anaconda to your PATH (this makes the Anaconda distribution the default Python).\nClose the terminal window."
  },
  {
    "objectID": "installation-and-setup.html#obtain-lesson-materials",
    "href": "installation-and-setup.html#obtain-lesson-materials",
    "title": "1  Installation & Setup",
    "section": "1.3 Obtain lesson materials",
    "text": "1.3 Obtain lesson materials\n\nDownload ??-data.zip and ??-code.zip (see Course Materials).\nCreate a folder called intro-python on your Desktop.\nMove downloaded files to intro-python.\nUnzip the files.\n\nYou should see two folders called data and code in the intro-python directory on your Desktop."
  },
  {
    "objectID": "installation-and-setup.html#launch-python-interface",
    "href": "installation-and-setup.html#launch-python-interface",
    "title": "1  Installation & Setup",
    "section": "1.4 Launch Python interface",
    "text": "1.4 Launch Python interface\nTo start working with Python, we need to launch a program that will interpret and execute our Python commands. Below we list several options. If you don’t have a preference, proceed with the top option in the list that is available on your machine. Otherwise, you may use any interface you like."
  },
  {
    "objectID": "installation-and-setup.html#option-a-jupyter-notebook",
    "href": "installation-and-setup.html#option-a-jupyter-notebook",
    "title": "1  Installation & Setup",
    "section": "1.5 Option A: Jupyter Notebook",
    "text": "1.5 Option A: Jupyter Notebook\nA Jupyter Notebook provides a browser-based interface for working with Python. If you installed Anaconda, you can launch a notebook in two ways:\n\nCommand line (Terminal)Anaconda Navigator\n\n\n\nNavigate to the data directory: Unix shell If you’re using a Unix shell application, such as Terminal app in macOS, Console or Terminal in Linux, or Git Bash on Windows, execute the following command:\ncd ~/Desktop/intro-python/data\nCommand Prompt (Windows) On Windows, you can use its native Command Prompt program. The easiest way to start it up is pressing Windows Logo Key+R, entering cmd, and hitting Return. In the Command Prompt, use the following command to navigate to the data folder:\ncd /D %userprofile%\\Desktop\\intro-python\\data\nStart Jupyter server: Unix shell\njupyter notebook\nCommand Prompt (Windows)\npython -m notebook\nLaunch the notebook by clicking on the “New” button on the right and selecting “Python 3” from the drop-down menu.\n\n\n\n\nLaunch Anaconda Navigator. It might ask you if you’d like to send anonymized usage information to Anaconda developers. Make your choice and click “Ok, and don’t show again” button.\nFind the “Notebook” tab and click on the “Launch” button. Anaconda will open a new browser window or tab with a Notebook Dashboard showing you the contents of your Home (or User) folder.\nNavigate to the data directory by clicking on the directory names leading to it. Desktop, intro-python, then data:\nLaunch the notebook by clicking on the “New” button and then selecting “Python 3”."
  },
  {
    "objectID": "installation-and-setup.html#option-b-ipython-interpreter",
    "href": "installation-and-setup.html#option-b-ipython-interpreter",
    "title": "1  Installation & Setup",
    "section": "1.6 Option B: IPython interpreter",
    "text": "1.6 Option B: IPython interpreter\nIPython is an alternative solution situated somewhere in between the plain-vanilla Python interpreter and Jupyter Notebook. It provides an interactive command-line based interpreter with various convenience features and commands. You should have IPython on your system if you installed Anaconda.\nTo start using IPython, execute:\nipython"
  },
  {
    "objectID": "installation-and-setup.html#option-c-plain-vanilla-python-interpreter",
    "href": "installation-and-setup.html#option-c-plain-vanilla-python-interpreter",
    "title": "1  Installation & Setup",
    "section": "1.7 Option C: plain-vanilla Python interpreter",
    "text": "1.7 Option C: plain-vanilla Python interpreter\nTo launch a plain-vanilla Python interpreter, execute:\npython\nIf you are using Git Bash on Windows, you have to call Python via winpty:\nwinpty python\nThe instructions on this page were adapted from the setup instructions of the Software Carpentries “Programming with Python” course and their Workshop Template Python installation instructions, both released under the Creative Commons Attribution license. Changes to the material were made, and can be tracked in the Git repository associated with this course."
  },
  {
    "objectID": "course-materials.html#zipped-file",
    "href": "course-materials.html#zipped-file",
    "title": "Workshop Materials",
    "section": "Zipped File",
    "text": "Zipped File\nThe following zipped file contains the notebooks and data required for the workshop.\n\nlink to zipped file\n\nDon’t forget to extract the contents of the zipped file after downloading!"
  },
  {
    "objectID": "course-materials.html#notebooks",
    "href": "course-materials.html#notebooks",
    "title": "Workshop Materials",
    "section": "Notebooks",
    "text": "Notebooks\nWe will be using the following Jupyter Notebooks to work on the exercises:\n\npython-101-exercises.ipynb\ndata-science-with-pandas-exercises.ipynb"
  },
  {
    "objectID": "course-materials.html#data",
    "href": "course-materials.html#data",
    "title": "Workshop Materials",
    "section": "Data",
    "text": "Data\nThe data for this workshop is from the Portal Teaching Database. We will be using the following datasets:\n\nsurveys.csv\nspecies.csv\nplots.csv"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What is Python? Why do we like it so much more than R? Let’s get going!"
  },
  {
    "objectID": "Introduction_to_python_1.html#variables-values-and-their-types",
    "href": "Introduction_to_python_1.html#variables-values-and-their-types",
    "title": "2  Variables and printing output",
    "section": "2.1 Variables, values and their types",
    "text": "2.1 Variables, values and their types\nThe cell below contains Python code that can be executed by the Python interpreter. One of the most basic things that we can do with Python is to use it as a calculator:\n\n2+2\n\n4\n\n\nGreat, but there are many calculators. It gets more interesting when we use variables to store information. This is done with the = operator. In Python, variable names: - can include letters, digits, and underscores - cannot start with a digit - are case sensitive.\n\nx = 3.0\n\nOnce assigned, variables can be used in new operations:\n\ny = 2.0\nx + y\n\n5.0\n\n\nPython knows various types of data. Three common ones are:\n\ninteger numbers\nfloating point numbers\nstrings\n\n\ntext = \"Data Carpentry\"\nnumber = 42\npi_value = 3.14159265358\n\nIn the example above, three variables are assigned. Variable number is an integer number with a value of 42 while pi_value is a floating point number and text is of type string.\nUsing the type command, it is possible to check the data type of a variable.\n\ntext\n\n'Data Carpentry'\n\n\n\ntype(text)\n\nstr\n\n\n\nnumber\n\n42\n\n\n\ntype(number)\n\nint\n\n\n\npi_value\n\n3.14159265358\n\n\n\ntype(pi_value)\n\nfloat"
  },
  {
    "objectID": "Introduction_to_python_1.html#output-versus-printing",
    "href": "Introduction_to_python_1.html#output-versus-printing",
    "title": "2  Variables and printing output",
    "section": "2.2 Output versus printing",
    "text": "2.2 Output versus printing\nIn the above examples, most of the times output is printed directly below the cell, but not always the output is printed and not all operations are printed. The print command can be used to control what is printed when.\nNote, that text (strings) always has to be surrounded by \" or '.\n\nprint(\"Hello World\")\n\nHello World\n\n\nIn the example below we first print the value of the variable number using the print command, and then call the variable:\n\nprint(number)\nnumber\n\n42\n\n\n42\n\n\nNow we do it the other way around:\n\nnumber\nprint(number)\n\n42\n\n\nWhen not using the print command, only the output of the last operation in the input cell is printed. If the last operation is the assignment of a variable, nothing will be printed.\nIn general print is the only way to print output to the screen when you are not working in an interactive environment like Jupyter (as we are doing now).\nRule of thumb: use the normal output for quick checking the output of an operation while developing in your Jupyter notebook, use print for printing output that still needs to be there in the future while your scripts get more complicated."
  },
  {
    "objectID": "Introduction_to_python_1.html#exercises",
    "href": "Introduction_to_python_1.html#exercises",
    "title": "2  Variables and printing output",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\nNow go to the Jupyter Dashboard in your internet browser and navigate to the course materials and open the notebook morning_exercises.ipynb\nIf Jupyter Dashboard is not there, check Installation & Setup for instructions to start the Jupyter Dashboard.\nDo Exercise 0 and after that come back to this document to continue with the following chapter Operators and Built-in Functions"
  },
  {
    "objectID": "Introduction_to_python_2.html#mathematical-operations",
    "href": "Introduction_to_python_2.html#mathematical-operations",
    "title": "3  Operators and built-in functions",
    "section": "3.1 Mathematical operations",
    "text": "3.1 Mathematical operations\nIn Python you can do a wide variety of mathematical operations. A few examples:\n\nsumming = 2 + 2\nmultiply = 2 * 7\npower = 2 ** 16\nmodulo = 13 % 5\n\nprint(\"Sum: \", summing)\nprint(\"Multiply: \", multiply)\nprint(\"Power: \", power)\nprint(\"Modulo: \", modulo)\n\nSum:  4\nMultiply:  14\nPower:  65536\nModulo:  3\n\n\nOnce we have data stored in variables, we can use the variables to do calculations.\n\nnumber = 42\npi_value = 3.14159265358\n\noutput = number * pi_value\nprint(output)\n\n131.94689145036"
  },
  {
    "objectID": "Introduction_to_python_2.html#built-in-python-functions",
    "href": "Introduction_to_python_2.html#built-in-python-functions",
    "title": "3  Operators and built-in functions",
    "section": "3.2 Built-in Python functions",
    "text": "3.2 Built-in Python functions\nTo carry out common tasks with data and variables in Python, the language provides us with several built-in functions. Examples of built-in functions that we already used above are print and type.\nCalling a function When we want to make use of a function (referred to as calling the function), we type the name of the function followed by parentheses. Between the parentheses we can pass arguments.\nArguments We typically provide a function with ‘arguments’ to tell python which values or variables are used to perform the body of the function. In the example below type is the function name and pi_value is the argument.\n\ntype(pi_value)\n\nfloat\n\n\nOther useful built-in functions are abs(), max(), min(), range(). Find more built-in functions here.\n\nmax([1,2,3,2,1])\n\n3"
  },
  {
    "objectID": "Introduction_to_python_2.html#boolean-values-logical-expressions-and-operators",
    "href": "Introduction_to_python_2.html#boolean-values-logical-expressions-and-operators",
    "title": "3  Operators and built-in functions",
    "section": "3.3 Boolean values, Logical expressions and operators",
    "text": "3.3 Boolean values, Logical expressions and operators\nIn programming you often need to know if something is True or False. True and False are called Boolean values and have their own data type (bool so they are not of type str!!). True and False are the only two Boolean values.\n\na = True\na\n\nTrue\n\n\n\nb = False\nb\n\nFalse\n\n\n\ntype(a)\n\nbool\n\n\nComparison operators (e.g. &gt;, &lt;, ==) are used in an expression to compare two values. The result of this expression is either True or False. Why this is useful we will show later (see if-statements).\n\n3 &gt; 4\n\nFalse\n\n\n3 &gt; 4 is an example of a ‘logical expression’ (also known as condition), where &gt; is the comparison operator.\n\n4 &gt; 3\n\nTrue\n\n\n== is another comparison operator to check if two values or variables are the same. If this is the case it will return True\n\nfour = 4          # first we assign the integer 4 to a variable\nfour == 4         # then we check if it is equal to 4\n\nTrue\n\n\n!= is used to check if two values or variable are not the same. If this is the case it will return True\n\nprint(\"Four is not equal to 5: \", four != 5)\nprint(\"Four is not equal to 4: \", four != 4)\n\nFour is not equal to 5:  True\nFour is not equal to 4:  False\n\n\nand, or and not are ‘logical operators’, and are used to join two logical expressions (or revert a logical expression in the case of not) to create more complex conditions.\nand will return True if both expression on either side are True.\n\na = True\nb = True\na and b\n\nTrue\n\n\n\na = True\nb = False\na and b\n\nFalse\n\n\n\n4 &gt; 3 and 9 &gt; 3\n\nTrue\n\n\nor is used to check if at least one of two logical expressions are True. If this is the case it will return True.\n\n3 &gt; 4 or 9 &gt; 3\n\nTrue\n\n\n\n4 &gt; 3 or 9 &gt; 3\n\nTrue\n\n\nIn the last three examples you can see that multiple expressions can be combined in a single line of Python code. Python evaluates the expressions one by one. 4 &gt; 3 would return True, 9 &gt; 3 would return True, so 4 &gt; 3 or 9 &gt; 3 would translate to True or True.\nIt is also possible to assign the output of an expression to a variable:\n\ngreater = 3 &gt; 4\nprint(\"3 &gt; 4 : \", greater)\n\n3 &gt; 4 :  False\n\n\nThe not operator can be used to reverse the Boolean value. If you apply not to an expression that evaluates to True, then you get False as a result. If you apply not to an expression that evaluates to False, then you get True as a result:\n\nnot 4 &gt; 3\n\nFalse\n\n\nLogical operators bind variables with different strengths. The and is stronger than the or and gets evaluated first in a boolean expression. So a or b and c will be evaluated like a or (b and c), while in (a or b) and c first the value of the or is evaluated and then combined with and c. This leads to a different result.\n\na = True\nb = True\nc = False\nprint(\"This expression 'a or b and c' evalutes to \", a or b and c)\nprint(\"And this is the same as 'a or (b and c)')\", a or (b and c))\nprint(\"But this expression evaluates '(a or b) and c' first the 'or' and generates:\", (a or b) and c)\n\nThis expression 'a or b and c' evalutes to  True\nAnd this is the same as 'a or (b and c)') True\nBut this expression evaluates '(a or b) and c' first the 'or' and generates: False"
  },
  {
    "objectID": "Introduction_to_python_2.html#exercises",
    "href": "Introduction_to_python_2.html#exercises",
    "title": "3  Operators and built-in functions",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\nNow go back to your browser to morning_exercises.ipynb and continue with exercises 1-3.\nWhen you finished the exercises, continue to chapter Data types, if-statements and for loops"
  },
  {
    "objectID": "Introduction_to_python_3.html#the-if-statement",
    "href": "Introduction_to_python_3.html#the-if-statement",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.1 The if-statement",
    "text": "4.1 The if-statement\nIf statements can be used to perform tasks only when a certain condition is met.\n\nnum = 101\n\nif num &gt; 100:\n    print('number is greater than 100')\n\nnumber is greater than 100\n\n\nAs you can see, the line print(...) starts with 4 spaces indentation. In Python indentation is very important. Python uses indentation to determine which lines of code belong to what part of the code. This is mostly important when defining e.g. if-statements, for loops or functions. After the if condition, all lines with indentation are only performed when the if-condition is met.\n\nnum = 99\nif num &gt; 100:\n    print('This line is only executed when num &gt; 100')\n    print('This line is only executed when num &gt; 100')\n    \n    print('This line is only executed when num &gt; 100')\n    \nprint('This line is always executed')\n\nThis line is always executed\n\n\nIt is also possible to specify a task that is performed when the condition is not met using else (note the use of indentation):\n\nnum = 37\n\nif num &gt; 100:\n    print('number is greater than 100')\nelse:\n    print('number is not greater than 100')\n\nprint('done')\n\nnumber is not greater than 100\ndone\n\n\nAn if ... else statement can be extended with (one or more) elif to specify more tasks that need to be performed on other conditions. These extended if ... else statements always start with if followed by (one or more) elif. When an else statement is included it is always the last statement.\nOrder matters: The statements (or conditions) are checked in order from top to bottom and only the task belonging to the first condition that is met is being performed.\n\nnum = -3\n\nif num &gt; 0:\n    print(num, 'is positive')\nelif num == 0:\n    print(num, 'is zero')\nelse:\n    print(num, 'is negative')\n\n-3 is negative\n\n\nAlong with the &gt; and == comparison operators that we have already used for comparing values in our logical expressions above, there are a few more options to know about:\n\n&gt;: greater than\n&lt;: less than\n==: equal to\n!=: does not equal\n&gt;=: greater than or equal to\n&lt;=: less than or equal to\n\nWe can combine logical statements using and and or in more complex conditions in if statements.\n\nif (1 &lt; 0) or (1 &gt;= 0):\n    print('at least one the above logical statements is true')\n\nat least one the above logical statements is true\n\n\nWhile and is only true if both parts are true\n\nif (1 &lt; 0) and (1 &gt;= 0):\n    print('both tests are true')\nelse:\n    print('at least one of the tests is not true')\n\nat least one of the tests is not true"
  },
  {
    "objectID": "Introduction_to_python_3.html#lists-and-tuples",
    "href": "Introduction_to_python_3.html#lists-and-tuples",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.2 Lists and Tuples",
    "text": "4.2 Lists and Tuples\nUntil now we have worked with values and variables that hold one value or string. Now we will go into other data types that can combine multiple values or strings.\nLists are common data structures to hold a sequence of elements. We can create a list by putting values inside square brackets and separating the values with commas.\n\nnumbers = [1, 2, 3]\nprint(numbers)\n\n[1, 2, 3]\n\n\nEach element can be accessed by an index. The index of the first element in a list in Python is 0 (in some other programming languages that would be 1).\n\nprint(\"The first element in the list numbers is: \", numbers[0])\n\nThe first element in the list numbers is:  1\n\n\n\ntype(numbers)\n\nlist\n\n\nA total number of items in a list is called the ‘length’ and can be calculated using the len() function.\n\nlen(numbers)\n\n3\n\n\nYou can do various things with lists. E.g. it is possible to sum the items in a list (when the items are all numbers)\n\nprint(\"The sum of the items in the list is:\", sum(numbers))\nprint(\"The mean of the items in the list is:\", sum(numbers)/len(numbers))\n\nThe sum of the items in the list is: 6\nThe mean of the items in the list is: 2.0\n\n\nWhat happens here:\n\nnumbers[3]\n\nIndexError: list index out of range\n\n\nThis error is expected. The list consists of three items, and the indices of those items are 0, 1 and 2.\n\nnumbers[-1]\n\n3\n\n\nYes, we can use negative numbers as indices in Python. When we do so, the index -1 gives us the last element in the list, -2 the second to last, and so on. Because of this, numbers[2] and numbers[-1] point to the same element.\n\nnumbers[2] == numbers[-1]\n\nTrue\n\n\nIt is also possible to combine strings in a list:\n\nwords = [\"cat\", \"dog\", \"horse\"]\nwords[1]\n\n'dog'\n\n\n\ntype(words)\n\nlist\n\n\n\nif type(words) == type(numbers):\n    print(\"these variables have the same type!\")\n\nthese variables have the same type!\n\n\nIt is also possible to combine values of different type (e.g. strings and integers) in a list\n\nnewlist = [\"cat\", 1, \"horse\"]\n\nThe type of the variable newlist is list. The elements of the list have their own data type:\n\ntype(newlist[0])\n\nstr\n\n\n\ntype(newlist[1])\n\nint\n\n\nIt is possible to add numbers to an existing list using list.append()\n\nnumbers.append(4)\nprint(numbers)\n\n[1, 2, 3, 4]\n\n\nUsing the index of an item, you can replace the item in a list:\n\nnumbers[2] = 333\nprint(numbers)\n\n[1, 2, 333, 4]\n\n\nNow what do you do if you do not know the index but you know the value of an item that you want to find in a list. How to find out at which position the value is listed?\n\nindex = newlist.index(\"cat\")\nprint(\"'cat' can be found at index\", index)\nprint(newlist[index])\n\n'cat' can be found at index 0\ncat\n\n\nA tuple is similar to a list in that it’s a sequence of elements. However, tuples can not be changed once created (they are “immutable”). Tuples are created by placing comma-separated values inside parentheses () (instead of square brackets []).\n\n# Tuples use parentheses\na_tuple = (1, 2, 3)\nanother_tuple = ('blue', 'green', 'red')\n\n# Note: lists use square brackets\na_list = [1, 2, 3]\n\n\na_list[1] = 5\nprint(a_list)\n\n[1, 5, 3]\n\n\n\na_tuple[1] = 5\nprint(a_tuple)\n\nTypeError: 'tuple' object does not support item assignment\n\n\nHere we see that once the tuple is created, we cannot replace any of the values inside of the tuple.\n\ntype(a_tuple)\n\ntuple"
  },
  {
    "objectID": "Introduction_to_python_3.html#dictionaries",
    "href": "Introduction_to_python_3.html#dictionaries",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.3 Dictionaries",
    "text": "4.3 Dictionaries\nA dictionary is another way to store multiple items into one object. In dictionaries, however, this is done with keys and values. This can be useful for several reasons, one example is to store model settings, parameters or variable values for multiple scenarios.\n\nmy_dict = {'one': 'first', 'two': 'second'}\nmy_dict\n\n{'one': 'first', 'two': 'second'}\n\n\nWe can access dictionary items by their key:\n\nmy_dict['one']\n\n'first'\n\n\nAnd we can add new key-value pairs like that:\n\nmy_dict['third'] = 'three'\nmy_dict\n\n{'one': 'first', 'two': 'second', 'third': 'three'}\n\n\nDictionary items are key-value pairs. The keys are changeable and unique. The values are changable, but not necessarily unique.\n\nmy_dict['two'] = 'three'\nmy_dict\n\n{'one': 'first', 'two': 'three', 'third': 'three'}\n\n\n\nprint(\"Dictionary keys: \", my_dict.keys())\nprint(\"Dictionary values: \", my_dict.values())\nprint(\"Dictionary items (key, value): \", my_dict.items())\n\nDictionary keys:  dict_keys(['one', 'two', 'third'])\nDictionary values:  dict_values(['first', 'three', 'three'])\nDictionary items (key, value):  dict_items([('one', 'first'), ('two', 'three'), ('third', 'three')])"
  },
  {
    "objectID": "Introduction_to_python_3.html#for-loops",
    "href": "Introduction_to_python_3.html#for-loops",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.4 For loops",
    "text": "4.4 For loops\nLet’s have a look at our list again. One way to print each number is to use three print statements:\n\nnumbers = [5, 6, 7]\nprint(numbers[0])\nprint(numbers[1])\nprint(numbers[2])\n\n5\n6\n7\n\n\nA more efficient (less typing) and reliable way to print each element of a list is to loop over the list using a for loop:\n\nfor item in numbers:\n    print(item)\n\n5\n6\n7\n\n\nThe improved version uses a for loop to repeat an operation — in this case, printing — once for each item in a sequence. Note that (similar to if statements) Python needs indentation (4 whitespaces) to determine which lines of code are part of the for loop.\nIf we want to also get the index, we can use the built-in function enumerate:\n\nwords = [\"cat\", \"dog\", \"horse\"]\n\nfor index, item in enumerate(words):\n    print(index)\n    print(item)\n\n0\ncat\n1\ndog\n2\nhorse\n\n\nFor loops can also be used with dictionaries. Let’s take our dictionary from the previous section and inspect the dictionary items\n\nfor item in my_dict.items():\n    print(item, \"is of type\", type(item))\n\n('one', 'first') is of type &lt;class 'tuple'&gt;\n('two', 'three') is of type &lt;class 'tuple'&gt;\n('third', 'three') is of type &lt;class 'tuple'&gt;\n\n\nWe can extract the keys and values from the items directly in the for statement:\n\nfor key, value in my_dict.items():\n    print(key, \"-&gt;\", value)\n\none -&gt; first\ntwo -&gt; three\nthird -&gt; three"
  },
  {
    "objectID": "Introduction_to_python_3.html#exercises",
    "href": "Introduction_to_python_3.html#exercises",
    "title": "4  Data types, if-statements and for-loops",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\nNow go back to your browser to morning_exercises.ipynb and continue with exercises 4-7.\nWhen you finished the exercises, continue to chapter Write your own Python function"
  },
  {
    "objectID": "Introduction_to_python_4.html#functions",
    "href": "Introduction_to_python_4.html#functions",
    "title": "5  Write your own Python function",
    "section": "5.1 Functions",
    "text": "5.1 Functions\nWe have already seen some built-in functions: e.g. print, type, len. And we have seen special functions that belong to a variable (python object) like my_dict.items() and my_list.append(). There are more built-in functions e.g. for mathematical operations:\n\nnumbers = [5, 6, 7]\nsum(numbers)\n\n18\n\n\nPlease refer to https://docs.python.org/3/library/functions.html for more built-in functions.\n\n5.1.1 Writing own functions\nWe will now turn to writing own functions. When should you write your own function?\n1. If the functionality is not covered by an out-of-the-box function like the built-in functions or another python package\n2. When code is getting pretty long, you can split it up into logical and reusable units\n3. When code is often reused, e.g. you are reading in tens of spreadsheets and you need to clean them all in the same way. Instead of typing the line of code over and over again, it is more elegant and looks cleaner to create a function.\n4. When code may be reused outside your current project. Scripts and the functions in a script can be imported in other scripts to be able to reuse them.\nA big advantage of not having duplicate code inside your script or in multiple scripts is that when you want to make a slight modification to a function, you only have to do this modification in one place, instead of multiple lines that are doing more or less similar things.\nPython provides for this by letting us define things called ‘functions’. Let’s start by defining a function fahr_to_celsius that converts temperatures from Fahrenheit to Celsius:\n\ndef fahr_to_celsius(temp_fahrenheit):\n    temp_celsius = (temp_fahrenheit - 32) * (5/9)\n    return temp_celsius\n\nThe function definition opens with the keyword def followed by the name of the function fahr_to_celsius and a parenthesized list of variables (in this case only one temp_fahrenheit). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value.\nWhen we call the function, the values we pass to it as arguments are assigned to the variables in the function definition so that we can use them inside the function. Inside the function, we use a return statement to send a result back to whoever asked for it.\nLet’s try running our function.\n\nfahr_to_celsius(98)\n\n36.66666666666667\n\n\n\nprint('freezing point of water:', fahr_to_celsius(32), 'C')\nprint('boiling point of water:', fahr_to_celsius(212), 'C')\n\nfreezing point of water: 0.0 C\nboiling point of water: 100.0 C\n\n\nHere we directly passed a value to the function. We can also call the function with a variable:\n\na = 0\nprint(fahr_to_celsius(a))\n\n-17.77777777777778\n\n\nWhat happens if you pass a variable name that is not defined yet?\n\nprint(fahr_to_celsius(b))\n\nNameError: name 'b' is not defined"
  },
  {
    "objectID": "Introduction_to_python_4.html#exercises",
    "href": "Introduction_to_python_4.html#exercises",
    "title": "5  Write your own Python function",
    "section": "5.2 Exercises",
    "text": "5.2 Exercises\nNow go back to your browser to morning_exercises.ipynb and continue with exercises 8 and 9.\nWhen you finished the exercises, continue to the afternoon session"
  },
  {
    "objectID": "data-science-with-pandas-1.html",
    "href": "data-science-with-pandas-1.html",
    "title": "6  Data Science with Pandas (part 1: Introduction and basic statistics)",
    "section": "",
    "text": "7 Data Science with pandas\nData analysis heavily depends on the characteristics of the chosen research subject, but we can still identify main common data analysis steps as follows: - Define the problem or objective (research question); - Design/perform experiments and collect data; - Data Exploration and Visualization; - Data Cleaning and Preparation; - Data Analysis and Modelling; - Result Interpretation; - Reporting and Visualization.\nWhile tasks like finding a relevant research question and interpreting data are challenges for the critical and analytical skills of the single researcher, all the tasks related to data manipulation, from data reduction and statistical analysis to modelling and visualization, require a systematic use of a quite large variety of software. No need to say that implementing such software yourself from scratch would require a huge amount of time, without even mentioning all the issues related to software mantainance. Luckily for us, people have already devoloped software with specific tools for data analysis. These tools are organised into packages or libraries, they usually are openly available (according to the programming language you use), and they keep to be mantained and developed by a huge community of users. In particular, for python programmers, the most popular data analysis library is Pandas.\nThe python library Pandas is a popular open-source data analysis and data manupulation library for Python that started to be developed in 2008. The library is based on two main data structures: Series (1D, similar to numpy arrays) and DataFrame (2D labeled arrays). Why shall we use Series and DataFrame when we can work with numpy arrays and matrices? Because Pandas has been designed specifically for data manipulation and analysis tasks, providing tools and functions for data cleaning, data transformation, data aggregation, and data visualization, among other data-related tasks. Adopting Pandas, we can run through all the main technical tasks in our data analysis pipeline mainly using a single python library.\nPandas is widely used in data science, machine learning, and data analysis tasks, as it provides powerful tools for data handling and manipulation, making it easier to work with a large variety of data. Furthermore, it well integrates with other Python libraries for data analysis, machine learning, and statistical analysis, such as NumPy, Scikit-Learn, and StatsModels.\nOverall, Pandas is a powerful and flexible library for data analysis and manipulation in Python, widely used in various domains including data science, finance, business, and research.\nIn this session we will explore the main features of Pandas going through the main steps of standard data analysis. For this purpose, we will be using data from the Portal Project Teaching Database: real world example of life-history, population, and ecological data and, occasionally, small ad hoc dataset to exaplain DataFrame operations."
  },
  {
    "objectID": "data-science-with-pandas-1.html#preliminaries",
    "href": "data-science-with-pandas-1.html#preliminaries",
    "title": "6  Data Science with Pandas (part 1: Introduction and basic statistics)",
    "section": "7.1 Preliminaries",
    "text": "7.1 Preliminaries\nBefore we start our journey into Pandas functionalities, there are some preliminary operations to run.\nThe Pandas library is not native of python (it is not automatically loaded when you start running python), it needs to be installed and loaded. Assuming you already installed it, let’s start importing the Pandas library and checking our installed version.\n\nimport pandas as pd\nprint(pd.__version__)\n\n1.5.3\n\n\nThe path of our data files will be specified relatively to the main project folder, so, in order to properly upload our dataset, it’s important to check we are working in there. In order to do that, we will load another library, os, containing all sort of tools to interact with our operating system. The function os.getcwd(), in particular, returns the current working directory (cwd).\n\nimport os\ncwd = os.getcwd()\nprint(cwd)\n\n/home/runner/work/workshop-introduction-to-python/workshop-introduction-to-python/book\n\n\nIf the current local directory is /workshop-introduction-to-python, where  is whatever directory you chose to download and unzip the course material, you are in the right place.\nLet’s store the relative path of our data into a variable and let’s check if the data file actually exists using the function os.path.exists()\n\ndata_file = '../course_materials/data/surveys.csv'\nprint(os.path.exists(data_file))\n\nTrue\n\n\nIf the result is True, we are all set up to go!\n\n7.1.1 Reading data and quick look (data exploration)\nThe very first operation we will perform is loading our data into a Pandas DataFrame (from now on, only DataFrame). A DataFrame is a Pandas core object, i.e. a key component that provides the primary functionality or core features of the entire library. We can convert our data into DataFrame using Pandas functions, they can read a quite large variety of formats like the comma-separated values (CSV) file format. In CSV files data values are separated by a comma (“,”), but files where values are separated by a semicolon (“;”), space (” “) or tab (”) can also have a .csv extention. Once specified the spacer, Pandas can read all these files with the function pd.read_csv(). The default spacer is a comma, so in our specific case there is no need to specify the separator.\n\nsurveys_df = pd.read_csv(data_file)\n\nprint(type(data_file))\nprint(type(surveys_df))\n\n&lt;class 'str'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nThe variable data_file is a string (a word) specifying the location of our file. We use it as the first argument of the function pd.read_csv() that, indeed, reads the file and returns a DataFrame. In Jupyter Notebook or Jupyter Lab you can visualise the DataFrame simply writing its name in a code cell and running the cell (in the same way you would display the value of any variable). Let’s have a look at our just created DataFrame:\n\nsurveys_df\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n35549 rows × 9 columns\n\n\n\nBy looking at the DataFrame we can finally understand what a DataFrame actually is: a 2-dimensional data structure storing different types of variables in columns. Ever DataFrame has an Index (starting from 0) referring to the rows of the table. Columns can be addressed either by their names or by their index, where 0 corresponds to the very first column.\nJupyter does something else for us, it does not show the entire DataFrame as it is too big, instead it shows the first and last 5 rows separated by … . Simply typing the name of our DataFrame in Jupyter is a great way to start looking at how our data looks like, get familiar with all the columns names, values, and type, and, most importantly, to check for the presence of the infamous NaN (not a number), as these not numeric values are the worst enemies of data analysis scripts.\n\nTRY IT YOURSELF: Type the following commands and check out the outputs. Can you tell what each command does? What is the difference between commands with and without parenthesis?\n\nsurveys_df.shape\nsurveys_df.columns\nsurveys_df.index\nsurveys_df.dtypes\nsurveys_df.head(&lt;try_various_integers_here&gt;)\nsurveys_df.tail(&lt;try_various_integers_here&gt;)"
  },
  {
    "objectID": "data-science-with-pandas-1.html#basic-statistics",
    "href": "data-science-with-pandas-1.html#basic-statistics",
    "title": "6  Data Science with Pandas (part 1: Introduction and basic statistics)",
    "section": "7.2 Basic statistics",
    "text": "7.2 Basic statistics\nIt’s time to perform some basic statistics on our dataset. There is nothing to worry about, Pandas has been specifucally designed for data analysis, so Pandas datastructures have plenty of attributes and methods for our needs (you can always visualise detailed information about attributes and methods of any python object running help(&lt;object&gt;)). Let’s first visualise all the column names in the DataFrame\n\nprint(surveys_df.columns)\n\nIndex(['record_id', 'month', 'day', 'year', 'plot_id', 'species_id', 'sex',\n       'hindfoot_length', 'weight'],\n      dtype='object')\n\n\nWe can use the loop statements we learned about this morning to print the DataFrame column one by one:\n\nfor column in surveys_df.columns:\n    print(column)\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\nLet’s select the column weight in our DataFrame and let’s run some statistics on it\n\nweight = surveys_df['weight']\nprint(type(weight))\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nweight is another Pandas core object, a Series. You can think at a Series as a python list, or numpy array, with extra powers. One of these extra powers allows us to make a simple plot, just to have a first glance at the data. As this is the first plot of this session, we specify the command %matplotlib inline, in this way jupyter lab or notebook will print the plot in the current notebook instead of opening a new window.\n\n%matplotlib inline\nweight.plot(kind='hist')\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\nDid you notice how easy it was to obtain a summary plot of a column of our DataFrame? We can repeat the same for every column with a single line of code.\n\nsurveys_df['hindfoot_length'].plot(kind='hist')\n\n&lt;AxesSubplot: ylabel='Frequency'&gt;\n\n\n\n\n\n\nTRY IT YOURSELF: Perform some basic statistics on the weight column. Can you tell what each method listed below does? Look at our explorative plot, does the statistics make sense?\n\nweight.min()\nweight.max()\nweight.mean()\nweight.std()\nweight.counts()\nFor each Series we have, instead of running the methods above one by one, we can obtain a statistical summary using the method .describe(). Let’s get a statistical summary for the weight column.\n\nweight.describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\n\n\n.describe() is just one of the many Series and DataFrame methods. There are methods to remove duplicates, to sort values in a particular order, to filter data according to custom criteria, etc (you can always check the Pandas documentation for more information). One of these methods, .nunique(), returns the number of unique elements in a Series. Comparing this number with the length of the Series allows us to check if all the values of a column are unique.\n\nprint(len(surveys_df['plot_id']))\nprint(surveys_df['plot_id'].nunique())\n\n35549\n24\n\n\nIn this case the column plot_id has only 24 unique elements, while the total number of elements (computed applying the function len()) is 35549. We definetively have repeated values in that column. Checking if there are repeating elements in a DataFrame column is one of the most important operation in data analysis and introduces the next topic of this session: grouping."
  },
  {
    "objectID": "data-science-with-pandas-2.html#recap-load-the-data",
    "href": "data-science-with-pandas-2.html#recap-load-the-data",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.1 Recap: Load the data",
    "text": "7.1 Recap: Load the data\n\nimport pandas as pd\nsurveys_df = pd.read_csv('../course_materials/data/surveys.csv')\nsurveys_df.describe\n\n&lt;bound method NDFrame.describe of        record_id  month  day  year  plot_id species_id  sex  hindfoot_length  \\\n0              1      7   16  1977        2         NL    M             32.0   \n1              2      7   16  1977        3         NL    M             33.0   \n2              3      7   16  1977        2         DM    F             37.0   \n3              4      7   16  1977        7         DM    M             36.0   \n4              5      7   16  1977        3         DM    M             35.0   \n...          ...    ...  ...   ...      ...        ...  ...              ...   \n35544      35545     12   31  2002       15         AH  NaN              NaN   \n35545      35546     12   31  2002       15         AH  NaN              NaN   \n35546      35547     12   31  2002       10         RM    F             15.0   \n35547      35548     12   31  2002        7         DO    M             36.0   \n35548      35549     12   31  2002        5        NaN  NaN              NaN   \n\n       weight  \n0         NaN  \n1         NaN  \n2         NaN  \n3         NaN  \n4         NaN  \n...       ...  \n35544     NaN  \n35545     NaN  \n35546    14.0  \n35547    51.0  \n35548     NaN  \n\n[35549 rows x 9 columns]&gt;\n\n\nIn addition to learning about characteristics of our dataset as a whole, we may be interested in analyzing parts (subsets) of our data. For exampe we want to know how heavy our samples are:\n\nsurveys_df['weight'].describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\n\n\nWe can also extract one specific metric if we wish:\n\nsurveys_df['weight'].min()\nsurveys_df['weight'].max()\nsurveys_df['weight'].mean()\nsurveys_df['weight'].std()\nsurveys_df['weight'].count()\n\n32283"
  },
  {
    "objectID": "data-science-with-pandas-2.html#selecting-data-using-column-names",
    "href": "data-science-with-pandas-2.html#selecting-data-using-column-names",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.2 Selecting data using column names",
    "text": "7.2 Selecting data using column names\nIn the morning session we saw how to get specific values from dictionaries using keys. We can do the same with DataFrames, in fact we have already accessed the values in a column by the column name. In this section we will discover how to select values, slices of data and subsets of a DataFrame. There are two ways of selecting columns, we have already used the first:\n\nsurveys_df['species_id']\n\n0         NL\n1         NL\n2         DM\n3         DM\n4         DM\n        ... \n35544     AH\n35545     AH\n35546     RM\n35547     DO\n35548    NaN\nName: species_id, Length: 35549, dtype: object\n\n\n\nsurveys_df.species_id\n\n0         NL\n1         NL\n2         DM\n3         DM\n4         DM\n        ... \n35544     AH\n35545     AH\n35546     RM\n35547     DO\n35548    NaN\nName: species_id, Length: 35549, dtype: object\n\n\nHow can we now create a DataFrame that only consists of the two columns plot_id and species_id?\n\nsurveys_df[['plot_id', 'species_id']]\n\n\n\n\n\n\n\n\nplot_id\nspecies_id\n\n\n\n\n0\n2\nNL\n\n\n1\n3\nNL\n\n\n2\n2\nDM\n\n\n3\n7\nDM\n\n\n4\n3\nDM\n\n\n...\n...\n...\n\n\n35544\n15\nAH\n\n\n35545\n15\nAH\n\n\n35546\n10\nRM\n\n\n35547\n7\nDO\n\n\n35548\n5\nNaN\n\n\n\n\n35549 rows × 2 columns\n\n\n\nWhy the double [[..]]? What is the difference between surveys_df['plot_id'] and surveys_df[['plot_id']]? Let us have a closer look:\n\nprint(type(surveys_df['plot_id']))\nprint(type(surveys_df[['plot_id']]))\n\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nThe DataFrame is organised as a dictionary with the column names as keys and row numbers as keys for the values stored in a row. surveys_df['plot_id'] will give us the value behind the key plot_id, in our case the series of numbers. When we ask for the values behind plot_id and species_id we need to give the DataFrame a list of column names like we did with surveys_df[['plot_id', 'species_id']]. When we pass a list of column names to a DataFrame, Pandas will execute for us the following code so that we do not have to worry about that any longer:\n\ncol1 = surveys_df['plot_id']\ncol2 = surveys_df['species_id']\naggregatedData = pd.DataFrame(dict(col1 = col1, col2 = col2))\naggregatedData\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\n0\n2\nNL\n\n\n1\n3\nNL\n\n\n2\n2\nDM\n\n\n3\n7\nDM\n\n\n4\n3\nDM\n\n\n...\n...\n...\n\n\n35544\n15\nAH\n\n\n35545\n15\nAH\n\n\n35546\n10\nRM\n\n\n35547\n7\nDO\n\n\n35548\n5\nNaN\n\n\n\n\n35549 rows × 2 columns"
  },
  {
    "objectID": "data-science-with-pandas-2.html#slicing-subsets-of-rows",
    "href": "data-science-with-pandas-2.html#slicing-subsets-of-rows",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.3 Slicing subsets of rows",
    "text": "7.3 Slicing subsets of rows\nSlicing using the [] operator selects a set of rows and/or columns from a DataFrame. To slice out a set of rows, you use the following syntax: data[start:stop]. When slicing in pandas the start bound is included in the output. The stop bound is not included. The slicing stops before the stop bound. So if you want to select rows 0, 1 and 2 your code would look like this:\n\nsurveys_df[0:3]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n\n\n\n\n\nWe can select specific ranges of our data in both the row and column directions using either label or integer-based indexing. The respective functions for that are called loc (label-based indexing) and iloc (integer-based indexing).\nLet’s have a look at iloc first. where we use the index of a row and/or column to select it. In the example below we select the first three entries and the columns month, day and year (the second, third and fourth column, remember indexing starts at 0 on Python). The first range of numbers selects the rows, the second the columns:\n\n# iloc[row slicing, column slicing]\nsurveys_df.iloc[0:3, 1:4]\n\n\n\n\n\n\n\n\nmonth\nday\nyear\n\n\n\n\n0\n7\n16\n1977\n\n\n1\n7\n16\n1977\n\n\n2\n7\n16\n1977\n\n\n\n\n\n\n\nWe can achieve the same with the function loc, only instead of column indices, we use the column labels this time. So, we need to know the names of the columns:\n\nsurveys_df.loc[0:3, ['month', 'day', 'year']]\n\n\n\n\n\n\n\n\nmonth\nday\nyear\n\n\n\n\n0\n7\n16\n1977\n\n\n1\n7\n16\n1977\n\n\n2\n7\n16\n1977\n\n\n3\n7\n16\n1977\n\n\n\n\n\n\n\nAnd there is a third way: In a forst step we select the columns by their names surveys_df[['month', 'day', 'year']]. From the resulting DataFrame we then, in a second step, select the first three rows [0:3]. Putting the two steps together, the code looks like this:\n\nsurveys_df[['month', 'day', 'year']][0:3]\n\n\n\n\n\n\n\n\nmonth\nday\nyear\n\n\n\n\n0\n7\n16\n1977\n\n\n1\n7\n16\n1977\n\n\n2\n7\n16\n1977\n\n\n\n\n\n\n\n\n7.3.1 Interactive Part\nLet us further explore the loc and iloc functions as they are more powerful. Have a look at the examples below and predict their outcome before hitting enter.\n\n# Select all columns for rows of index values 0 and 10\nsurveys_df.loc[[0, 10], :]\n\n# What does this do?\nsurveys_df.loc[0, ['species_id', 'plot_id', 'weight']]\n\n# What happens when you type the code below?\nsurveys_df.loc[[0, 10, 35549], :]\n\nKeyError: '[35549] not in index'\n\n\nWe can also extract single values from our DataFrame:\n\n# data.iloc[row, column]\nsurveys_df.iloc[2, 6]\n\n'F'\n\n\n\n\n7.3.2 Summary: Selecting slices, rows and columns\nIn the first two methods we extract the column specifying its name. The third method is essentially identical to the first one as the 6th (index 5) element of the Series surveys_df.columns is species_id. The fourth method uses the method iloc to select all the rows of the 6th column.\n\n# By name\n# --------------------------------------\n# Method1\nplot_id_1 = surveys_df['species_id']\n\n# Method2\nplot_id_2 = surveys_df.species_id\n# --------------------------------------\n\n# By location\n# --------------------------------------\n# Method3\nplot_id_3 = surveys_df[surveys_df.columns[5]]\n\n# Method4\nplot_id_4 = surveys_df.iloc[:,5]\n# --------------------------------------\n\n\nExercise 3 to 5\nNow go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 3 to 5.\n\n\n7.3.3 Subsetting Data according to user-defined criteria\nWe can extract subsets of our DataFrame following the general syntax data_frame[&lt;condition_on_data&gt;]  is a conditional statement on the DataFrame content itself. You may think at the conditional statement as a question or query you ask to your DataFrame. Here there are some examples:\n\n# What are the data collected in the year 2002?\nsurveys_df[surveys_df.year == 2002]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n33320\n33321\n1\n12\n2002\n1\nDM\nM\n38.0\n44.0\n\n\n33321\n33322\n1\n12\n2002\n1\nDO\nM\n37.0\n58.0\n\n\n33322\n33323\n1\n12\n2002\n1\nPB\nM\n28.0\n45.0\n\n\n33323\n33324\n1\n12\n2002\n1\nAB\nNaN\nNaN\nNaN\n\n\n33324\n33325\n1\n12\n2002\n1\nDO\nM\n35.0\n29.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2229 rows × 9 columns\n\n\n\n\n# What are the data NOT collected in the year 2002?\nsurveys_df[surveys_df.year != 2002]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n33315\n33316\n12\n16\n2001\n11\nNaN\nNaN\nNaN\nNaN\n\n\n33316\n33317\n12\n16\n2001\n13\nNaN\nNaN\nNaN\nNaN\n\n\n33317\n33318\n12\n16\n2001\n14\nNaN\nNaN\nNaN\nNaN\n\n\n33318\n33319\n12\n16\n2001\n15\nNaN\nNaN\nNaN\nNaN\n\n\n33319\n33320\n12\n16\n2001\n16\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n33320 rows × 9 columns\n\n\n\n\n# What are the data NOT collected in the year 2002? (different syntax)\nsurveys_df[~(surveys_df.year == 2002)]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n33315\n33316\n12\n16\n2001\n11\nNaN\nNaN\nNaN\nNaN\n\n\n33316\n33317\n12\n16\n2001\n13\nNaN\nNaN\nNaN\nNaN\n\n\n33317\n33318\n12\n16\n2001\n14\nNaN\nNaN\nNaN\nNaN\n\n\n33318\n33319\n12\n16\n2001\n15\nNaN\nNaN\nNaN\nNaN\n\n\n33319\n33320\n12\n16\n2001\n16\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n33320 rows × 9 columns\n\n\n\nOur filtering conditions may be very specific, they can target different columns in the DataFrame, and they can be combined using the logical operator “&” which means and:\n\n# What are the data collected between 2000 and 2002 on female species?\nsurveys_df[(surveys_df.year &gt;= 2000) & (surveys_df.year &lt;= 2002) & (surveys_df.sex == 'F')]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n30158\n30159\n1\n8\n2000\n1\nPP\nF\n22.0\n17.0\n\n\n30160\n30161\n1\n8\n2000\n1\nPP\nF\n21.0\n17.0\n\n\n30164\n30165\n1\n8\n2000\n1\nPP\nF\n22.0\n15.0\n\n\n30168\n30169\n1\n8\n2000\n2\nPB\nF\n25.0\n24.0\n\n\n30171\n30172\n1\n8\n2000\n2\nNL\nF\n30.0\n137.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35539\n35540\n12\n31\n2002\n15\nPB\nF\n26.0\n23.0\n\n\n35540\n35541\n12\n31\n2002\n15\nPB\nF\n24.0\n31.0\n\n\n35541\n35542\n12\n31\n2002\n15\nPB\nF\n26.0\n29.0\n\n\n35542\n35543\n12\n31\n2002\n15\nPB\nF\n27.0\n34.0\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n\n\n2582 rows × 9 columns\n\n\n\nWe have also an operator for or. Below we filter for rows with collected data on female species in the year 2000 or 2002. “Give me all data where sex is Female and data is collected in 2000 or 2002”.\nThe method isin() allows to specify a range of “permitted” values for a certain column. Here it follows another example:\n\nsurveys_df[(surveys_df.year == 2000) & (surveys_df.sex == 'F') & (surveys_df.month.isin([1,3,4]))]\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n30158\n30159\n1\n8\n2000\n1\nPP\nF\n22.0\n17.0\n\n\n30160\n30161\n1\n8\n2000\n1\nPP\nF\n21.0\n17.0\n\n\n30164\n30165\n1\n8\n2000\n1\nPP\nF\n22.0\n15.0\n\n\n30168\n30169\n1\n8\n2000\n2\nPB\nF\n25.0\n24.0\n\n\n30171\n30172\n1\n8\n2000\n2\nNL\nF\n30.0\n137.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n30637\n30638\n4\n30\n2000\n20\nPP\nF\n22.0\n20.0\n\n\n30640\n30641\n4\n30\n2000\n20\nNL\nF\n30.0\nNaN\n\n\n30645\n30646\n4\n30\n2000\n24\nPP\nF\n20.0\n17.0\n\n\n30647\n30648\n4\n30\n2000\n17\nDM\nF\n36.0\n46.0\n\n\n30648\n30649\n4\n30\n2000\n17\nDO\nF\n36.0\n59.0\n\n\n\n\n156 rows × 9 columns"
  },
  {
    "objectID": "data-science-with-pandas-2.html#dataframe-cleaning",
    "href": "data-science-with-pandas-2.html#dataframe-cleaning",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.4 DataFrame Cleaning",
    "text": "7.4 DataFrame Cleaning\nA simple exploration of our DataFrame showed us that there are columns full of invalid values (NaN). One of the most important preliminary operations of data analysis is cleaning your data set, i.e. “getting rid” of non-numerical or non-character values. we want to make sure that our data only contains meaningful values.\nNow that we mastered selecting, slicing, and subsetting, we can easily clean our DataFrame with few lines of code. Let us have a look at the function isnull. It is a Pandas function which we imported at the beginning with import pandas as pd. Now we can call the functin like this:\n\npd.isnull(4)\n\nFalse\n\n\n\npd.isnull([1, 2, 3, '', dict(), None])\n\narray([False, False, False, False, False,  True])\n\n\nWe can pass single values or array-like values to the function. The function will then check for us whether each value is NaN (Not a Number) or None and return a boolean array. Note, that values like the empty string (a strin without any characters in it) or an empty dictionary etc will not count as null value, they do have a type, they only do not contain any values but they are something. null values in python are only NaN and None. When you read in tabular data into a DataFrame empty cells will be shown as NaN. None stands for the type NoneType, which we will not dive into further in this workshop.\nWith all that kowledge we can now detect null values in the column weight and do something about it. Let us have a look how many null values we can find:\n\nprint(\"Length of the full dataframe: \", len(surveys_df))\npd.isnull(surveys_df.weight) # boolean array indicating where null values are found\nsurveys_df[pd.isnull(surveys_df.weight)] # all lines that have a null value in the column weight\nlen(surveys_df[pd.isnull(surveys_df.weight)]) # length\n\nLength of the full dataframe:  35549\n\n\n3266\n\n\nAs you can see, in our whole dataset 3266 weight values are not usable. We need to do something with those values.\nAnother thing that would not make sense are negative weights. Let’s check whether the remaining 32283 values in the weight column are positive:\n\nlen(surveys_df[surveys_df.weight &gt; 0])\n\n32283\n\n\nAs we see, we have 32283 non-negative weighht values. The remaining 3266 values in the weight column are not set, so they are null. How can we impute the values? Let us have a look at the average weight:\n\nsurveys_df.weight.mean()\n\n42.672428212991356\n\n\nA smooth run, without errors or warnings. As we said several times, Pandas is a library designed for data analysis and when performing data analysis it is very common to deal with not numeric values. In particular, the .mean() method has an argument called skipna that when set True (default value, so we do not need to specify it) excludes NaN values. This means that, in this case, Pandas simply ignores whatever it is not numeric and it performs computations only on numeric values.\nIf we are not happy with Pandas default behaviour, we can manually decide which value to assign to cells that contain null values. One possible choice is setting them to zero. To do that, we just need to apply the method .fillna(&lt;value&gt;), where &lt;value&gt; is the number we want to substitute to the null value with (in our case, 0).\n\ncleaned_weight1 = surveys_df.weight.fillna(0)\ncleaned_weight_ave1 = cleaned_weight1.mean()\nprint(cleaned_weight_ave1)\n\n38.751976145601844\n\n\nYou see that when filling the null values with 0, the average weight decreases. This is because the mean is now computed on data with many more zeros compared to the previous one. Conscious of this problem, we may now choose a more appropriate value to “fill” our null values. How about we use the “clean” mean of our first computation?\n\ncleaned_weight2 = surveys_df.weight.fillna(surveys_df.weight.mean())\ncleaned_weight_ave2 = cleaned_weight2.mean()\nprint(cleaned_weight_ave2)\n\n42.672428212991356\n\n\nThis time we obtain exactly the same result of our first computation, this is because we substituted the null values with a mean computed excluding the null values.\n\nExercise 6 and 7\nNow go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 6 and 7."
  },
  {
    "objectID": "data-science-with-pandas-2.html#grouping",
    "href": "data-science-with-pandas-2.html#grouping",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.5 Grouping",
    "text": "7.5 Grouping\nWe often want to calculate summary statistics grouped by subsets or attributes within fields of our data. For example, we might want to calculate the average weight of all individuals per site.\nAs we have seen above we can calculate basic statistics for all records in a single column using the syntax below:\n\nsurveys_df['weight'].describe()\n\ncount    32283.000000\nmean        42.672428\nstd         36.631259\nmin          4.000000\n25%         20.000000\n50%         37.000000\n75%         48.000000\nmax        280.000000\nName: weight, dtype: float64\n\n\nIf we want to summarize by one or more variables, for example sex, we can use Pandas’ .groupby() method. Once we’ve created a groupby DataFrame, we can quickly calculate summary statistics by a group of our choice.\n\ngrouped_data = surveys_df.groupby('sex')\ngrouped_data.describe()\n\n\n\n\n\n\n\n\nrecord_id\nmonth\n...\nhindfoot_length\nweight\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\n15690.0\n18036.412046\n10423.089000\n3.0\n8917.50\n18075.5\n27250.00\n35547.0\n15690.0\n6.587253\n...\n36.0\n64.0\n15303.0\n42.170555\n36.847958\n4.0\n20.0\n34.0\n46.0\n274.0\n\n\nM\n17348.0\n17754.835601\n10132.203323\n1.0\n8969.75\n17727.5\n26454.25\n35548.0\n17348.0\n6.396184\n...\n36.0\n58.0\n16879.0\n42.995379\n36.184981\n4.0\n20.0\n39.0\n49.0\n280.0\n\n\n\n\n2 rows × 56 columns\n\n\n\nThe output is a bit overwhelming. Let’s just have a look at one statistical value, the mean, to understand what is happening here:\n\ngrouped_data.mean()\n\n/tmp/ipykernel_2287/1133710423.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  grouped_data.mean()\n\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nhindfoot_length\nweight\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\nF\n18036.412046\n6.587253\n15.880943\n1990.644997\n11.440854\n28.836780\n42.170555\n\n\nM\n17754.835601\n6.396184\n16.078799\n1990.480401\n11.098282\n29.709578\n42.995379\n\n\n\n\n\n\n\nWe see that the data is divided into two groups, one group where the value in the column sex equals “F” and another group where the value in the column sex equals “M”. The statistics is then calculated for all samples in that specific group for each of the columns in the dataframe. Note that samples annotated with sex equals NaN and column values with NaN are left out.\n\ngrouped_data = surveys_df.groupby(...)\ngrouped_data[...].mean()\n\nTypeError: 'ellipsis' object is not callable"
  },
  {
    "objectID": "data-science-with-pandas-2.html#structure-of-a-groupby-object",
    "href": "data-science-with-pandas-2.html#structure-of-a-groupby-object",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.6 Structure of a groupby object",
    "text": "7.6 Structure of a groupby object\nWe can investigate which rows are assigned to which group as follows:\n\nprint(type(grouped_data.groups)) # dictionary\nprint(\"Plot ids: \", grouped_data.groups.keys()) # keys are the unique values of the column we grouped by\nprint(\"Rows belonging to plot id \", 1, \": \", grouped_data.groups[1]) # values are row indexes \n\n&lt;class 'pandas.io.formats.printing.PrettyDict'&gt;\nPlot ids:  dict_keys(['F', 'M'])\n\n\nKeyError: 1"
  },
  {
    "objectID": "data-science-with-pandas-2.html#grouping-by-multiple-columns",
    "href": "data-science-with-pandas-2.html#grouping-by-multiple-columns",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.7 Grouping by multiple columns",
    "text": "7.7 Grouping by multiple columns\nNow let’s have a look at a more complex grouping example. We want an overview statistics of the weight of all females and males by plot id. So in fact we want to group by sex and by plot_id at the same time.\nThis will give us exactly 48 groups for our survey data: - female, plot id = 1 - female, plot id = 2 - … - female, plot id = 24 - male, plot id = 1 - … - male, plot id = 24\nWhy 48 groups? We have 24 unique values for plot_id. Per plot we have two groups of samples, female and male. Hence, the grouping returns 48 groups.\n\ngrouped_data = surveys_df.groupby(['sex', 'plot_id'])\ngrouped_data[\"weight\"].describe()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsex\nplot_id\n\n\n\n\n\n\n\n\n\n\n\n\nF\n1\n826.0\n46.311138\n33.240958\n5.0\n26.00\n40.0\n50.00\n196.0\n\n\n2\n954.0\n52.561845\n45.547697\n5.0\n25.00\n40.0\n51.00\n274.0\n\n\n3\n873.0\n31.215349\n30.687451\n4.0\n15.00\n23.0\n34.00\n199.0\n\n\n4\n850.0\n46.818824\n33.560664\n5.0\n28.00\n40.0\n47.00\n200.0\n\n\n5\n516.0\n40.974806\n36.396966\n5.0\n21.00\n35.0\n45.00\n248.0\n\n\n6\n721.0\n36.352288\n29.513333\n5.0\n19.00\n29.0\n41.00\n188.0\n\n\n7\n326.0\n20.006135\n17.895937\n6.0\n12.00\n17.0\n23.00\n170.0\n\n\n8\n817.0\n45.623011\n31.045426\n5.0\n25.00\n42.0\n50.00\n178.0\n\n\n9\n823.0\n53.618469\n35.572793\n6.0\n35.00\n43.0\n54.00\n177.0\n\n\n10\n138.0\n17.094203\n14.074820\n7.0\n10.00\n13.0\n20.00\n130.0\n\n\n11\n796.0\n43.515075\n29.627049\n5.0\n27.00\n40.0\n46.00\n208.0\n\n\n12\n1040.0\n49.831731\n43.790247\n6.0\n26.00\n41.0\n48.25\n264.0\n\n\n13\n610.0\n40.524590\n36.109806\n5.0\n21.00\n31.0\n42.00\n192.0\n\n\n14\n692.0\n47.355491\n29.563455\n5.0\n37.00\n43.0\n48.00\n211.0\n\n\n15\n467.0\n26.670236\n31.983137\n4.0\n12.50\n18.0\n26.00\n198.0\n\n\n16\n211.0\n25.810427\n20.902314\n4.0\n13.00\n21.0\n31.00\n158.0\n\n\n17\n874.0\n48.176201\n37.485528\n6.0\n27.00\n41.0\n49.00\n192.0\n\n\n18\n740.0\n36.963514\n35.184417\n5.0\n17.00\n28.5\n40.00\n212.0\n\n\n19\n514.0\n21.978599\n14.008822\n6.0\n12.00\n20.0\n29.00\n139.0\n\n\n20\n631.0\n52.624406\n55.257665\n5.0\n17.00\n30.0\n48.00\n220.0\n\n\n21\n596.0\n25.974832\n22.619863\n4.0\n11.00\n24.0\n31.00\n188.0\n\n\n22\n646.0\n53.647059\n38.588538\n5.0\n29.00\n39.0\n54.00\n161.0\n\n\n23\n163.0\n20.564417\n18.933945\n8.0\n12.00\n16.0\n23.00\n199.0\n\n\n24\n479.0\n47.914405\n49.112574\n6.0\n21.00\n33.0\n44.00\n251.0\n\n\nM\n1\n1072.0\n55.950560\n41.035686\n4.0\n37.00\n46.0\n54.00\n231.0\n\n\n2\n1114.0\n51.391382\n46.690887\n5.0\n24.00\n42.0\n50.00\n278.0\n\n\n3\n827.0\n34.163241\n40.260426\n5.0\n13.00\n23.0\n39.00\n250.0\n\n\n4\n1010.0\n48.888119\n32.254168\n4.0\n32.00\n44.5\n50.00\n187.0\n\n\n5\n573.0\n40.708551\n31.250967\n6.0\n21.00\n40.0\n49.00\n240.0\n\n\n6\n739.0\n36.867388\n30.867779\n6.0\n18.00\n31.0\n46.00\n241.0\n\n\n7\n303.0\n21.194719\n23.971252\n4.0\n11.00\n17.0\n23.00\n235.0\n\n\n8\n962.0\n49.641372\n34.820355\n5.0\n29.00\n45.0\n52.00\n173.0\n\n\n9\n984.0\n49.519309\n31.888023\n6.0\n37.00\n46.0\n50.00\n275.0\n\n\n10\n139.0\n19.971223\n25.061068\n4.0\n10.00\n12.0\n22.00\n237.0\n\n\n11\n994.0\n43.366197\n28.425105\n6.0\n25.00\n43.0\n49.00\n212.0\n\n\n12\n1174.0\n48.909710\n39.301038\n7.0\n25.25\n43.0\n50.00\n280.0\n\n\n13\n757.0\n40.097754\n31.753448\n6.0\n20.00\n34.0\n47.00\n241.0\n\n\n14\n1029.0\n45.159378\n25.272173\n5.0\n35.00\n44.0\n50.00\n222.0\n\n\n15\n401.0\n27.523691\n38.631271\n4.0\n10.00\n18.0\n25.00\n259.0\n\n\n16\n265.0\n23.811321\n14.663726\n5.0\n11.00\n20.0\n35.00\n61.0\n\n\n17\n1011.0\n47.558853\n34.082010\n4.0\n27.00\n45.0\n51.00\n216.0\n\n\n18\n607.0\n43.546952\n41.864279\n7.0\n18.00\n33.0\n48.00\n256.0\n\n\n19\n567.0\n20.306878\n12.553954\n4.0\n10.00\n19.0\n25.00\n100.0\n\n\n20\n588.0\n44.197279\n43.361503\n5.0\n17.00\n34.0\n47.00\n223.0\n\n\n21\n431.0\n22.772622\n18.984554\n4.0\n9.00\n19.0\n32.00\n190.0\n\n\n22\n648.0\n54.572531\n38.841066\n6.0\n31.00\n44.0\n53.00\n212.0\n\n\n23\n205.0\n18.941463\n17.979740\n4.0\n10.00\n12.0\n22.00\n131.0\n\n\n24\n479.0\n39.321503\n42.003947\n4.0\n17.00\n24.0\n45.00\n230.0"
  },
  {
    "objectID": "data-science-with-pandas-2.html#counting-and-plotting",
    "href": "data-science-with-pandas-2.html#counting-and-plotting",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.8 Counting and plotting",
    "text": "7.8 Counting and plotting\nAnother very useful outcome of grouping is the possibility of performing selective counting. For example, let’s see how to count the number of records per species. We just need to remember that each species has a unique ID and that records are identified by another ID stored in the column record ID. We will first group our data according to the species ID and then, for each group, we will count the number of records. Several consecutive operations that, once again, Pandas allows us to execute in a single line.\n\nspecies_counts = surveys_df.groupby('species_id')['record_id'].count()\nprint(type(grouped_species_counts))\nspecies_counts\n\nNameError: name 'grouped_species_counts' is not defined\n\n\nWe can also plot the information for better overview. We will learn more about plotting after the next chapter.\n\nspecies_counts.plot(kind='bar')\n\n&lt;AxesSubplot: xlabel='species_id'&gt;"
  },
  {
    "objectID": "data-science-with-pandas-2.html#summary-grouping",
    "href": "data-science-with-pandas-2.html#summary-grouping",
    "title": "7  Data Science with Pandas (part 2: Grouping, Indexing, Slicing, and Subsetting DataFrames)",
    "section": "7.9 Summary grouping",
    "text": "7.9 Summary grouping\nGrouping is one of the most common operation in data analysis. Data often consists of different measurements on the same samples. In many cases we are not only interested in one particular measurement but in the cross product of measurements. In the picture below we labeled samples with green lines, blue dots and red lines. We are now interested how these three different groups relate to each other given the all other measurements in the dataframe. Pandas’ groupby function gives us the means to compare these three groups with several built-in statistical methods.\n\n\n\nGrouping sketch\n\n\n\nExercise 8 to 10\nNow go to the Jupyter Dashboard in your internet browser and continue with the afternoon exercises 8 to 10.\nAfter you finished the exercises please come back to this document and continue with the following chapter."
  },
  {
    "objectID": "data-science-with-pandas-3.html#combining-data-frames",
    "href": "data-science-with-pandas-3.html#combining-data-frames",
    "title": "8  Data Science with Pandas (part 3: Combining DataFrames)",
    "section": "8.1 Combining data frames",
    "text": "8.1 Combining data frames\nIn many real life cases, you may find data saved into different files and, therefore, you may need to deal with several different pandas DataFrames. In the previous session, we saw how can we easily run statistical analysis on a single DataFrame, so, ideally, we would like to have all the relevant data for our analysis inside a single DataFrame.  In this session we will explore different ways of combining DataFrames into a single DataFrame.\nLet’s start loading the pandas library, reading two data sets into pandas DataFrames, and having a quick look at the tabular data: surveys.csv and species.csv\n\nimport pandas as pd\n\n\nsurveys_df = pd.read_csv(\"../data/surveys.csv\", keep_default_na=False, na_values=[\"\"])\nspecies_df = pd.read_csv(\"../data/species.csv\", keep_default_na=False, na_values=[\"\"])\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/surveys.csv'\n\n\n\nprint(surveys_df.info())\nprint('='*72)\nsurveys_df.head()\n\nNameError: name 'surveys_df' is not defined\n\n\n\nprint(species_df.info())\nprint('='*72)\nspecies_df.head()\n\nNameError: name 'species_df' is not defined\n\n\n\n8.1.1 Concatenating DataFrames\nThe first way we will combine DataFrames is concatenation, i.e. simply putting DataFrames one after the other either verically or horizontally. To concatenate two DataFrames you will use the function pd.concat, specifying as arguments the DataFrames to concatenate and axis=0 or axis=1 for vertical or horizontal concatenation, respectively.\nTo play a bit with DataFrame concatenation, we will use a subset of the DataFrames we just read. In particular, we will work with two sub-DataFrames obtained selecting the first and the last 10 rows of the surveys.csv dataset.\n\n# Subsetting data frames\nsurveys_df_sub_first10 = surveys_df.head(10)\nsurveys_df_sub_last10  = surveys_df.tail(10)\n\nNameError: name 'surveys_df' is not defined\n\n\nLet’s start with vertical stacking. In this case the two DataFrames are simply stacked on top of each other (remember to specify axis=0).\n\n\n\n\n# Stack the DataFrames on top of each other\nvertical_stack = pd.concat([surveys_df_sub_first10, surveys_df_sub_last10], axis=0)\n\nNameError: name 'surveys_df_sub_first10' is not defined\n\n\n\nprint(vertical_stack.info())\nprint('='*72)\nvertical_stack\n\nNameError: name 'vertical_stack' is not defined\n\n\nThe resulting DataFrame (vertical_stack) consists, as expected, of 20 rows. These are the result of the first and last 10 rows of out original DataFrame surveys_df. You may have noticed that the last ten rows have very high index, not consecutive with the first ten rows. This is because concatenation preserves the indices of the two original DataFrames. If you want a brand new set of indices for your concateneted DataFrame, simply resets the indices using the method .reset_index().\n\nvertical_stack.reset_index()\n\nNameError: name 'vertical_stack' is not defined\n\n\n\nTRY IT YOURSELF: In the given example of vertical concatenation, you concatenated two DataFrames with the same columns. What would happen if the two DataFrames to concatenate have different column number and names?\n\n\nCreate a new DataFrame using the last 10 rows of the species DataFrame;\n\n\nConcatenate vertically surveys_df_sub_first_10 and your just created DataFrame;\n\n\nPrint the concatenated DataFrame info on the screen. How may rows does it have? What happened to the columns? Can you tell, finally, what happens when you vertically concatenate two DataFrames with different columns?\n\n\nIt’s now time to try horizontal concatenation. In this case the two DataFrames are simply stacked one after one other (remember to specify axis=1).\n\n\n\nIn this case, as a result, we expect a DataFrame with the same number of rows of the original one (10 row) and twice the number of columns (18 columns).\n\n# Place the DataFrames side by side\nhorizontal_stack = pd.concat([surveys_df_sub_first10, surveys_df_sub_last10], axis=1)\n\nNameError: name 'surveys_df_sub_first10' is not defined\n\n\n\nprint(horizontal_stack.info())\nprint('='*72)\nhorizontal_stack\n\nNameError: name 'horizontal_stack' is not defined\n\n\nLooking at the result of our horizontal concatenation, we may realise that something went wrong. The total number of row on the resulting DataFrame is 20, that is not what we would expect from a horizontal stacking (we where expecting 10 rows, as the initial DataFrames). This happens because horizontal stacking is based on index and our two DataFrames have different indices (1-9 and 35539-35548 respectively). In order to properly stack the DataFrame we need first to reset the indeces of the second DataFrame so that they will match the ones of the first DataFrame.\n\nsurveys_df_sub_last10 = surveys_df_sub_last10.reset_index(drop=True)\n\nNameError: name 'surveys_df_sub_last10' is not defined\n\n\n\nsurveys_df_sub_last10\n\nNameError: name 'surveys_df_sub_last10' is not defined\n\n\nNow that the index has been reset, we can concatenate this DataFrame with the first 10 lines DataFrame.\n\nhorizontal_stack = pd.concat([surveys_df_sub_first10, surveys_df_sub_last10], axis=1)\n\nNameError: name 'surveys_df_sub_first10' is not defined\n\n\n\nprint(horizontal_stack.info())\nprint('='*72)\nhorizontal_stack\n\nNameError: name 'horizontal_stack' is not defined\n\n\n\nTRY IT YOURSELF: In the given example of horizontal concatenation, you first concatenated two DataFrame with different indices, then reset the indices of the second one. Base on the outcome of these two cases, try to answer the following questions:\n\n\nWhat happens when you concatenate horizontally two DataFrames with different indexing?\n\n\nWhat happens when you concatenate horizontally two DataFrames with the same columns?\n\n\nWhat happens when you try to select a column of the just horizontally concatenated DataFrame?&gt;\n\n\nHow can you select a specific columns among duplicates?\n\n\n\n\n\n8.1.2 Joining DataFrames\nConcatenating DataFrames seems a quite “brutal” operation, you simply merge them one after another either verically or horizontally. What about if you want to merge DataFrames according to the value contained in specific columns? The pandas function merge() performs an operation that in database language is called join, the join operation adds the content of one DataFrame to another. There are different types of joins, but the workflow to perform a join operation is always the same:\n\n\nYou identify a left and a right DataFrames, the ones you want to join;\n\n\nYou identify in both your left and righ DataFrame a column to join on;\n\n\nYou choose the type of join;\n\n\nYou perform the join running the function pd.merge() with the specified inputs and options.\n\n\nLet’s see some join example considering two tiny (few rows) DataFrames, our left DataFrame contains general data of European capitals, while our right DataFrame contains weather measuraments for some Dutch towns.\n\nleft_df = pd.read_csv(\"../data/EU_capitals_tiny.csv\", sep=\",\", header=0)\nright_df = pd.read_csv(\"../data/Netherlands_town_weather_tiny.csv\", sep=\",\", header=0)\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/EU_capitals_tiny.csv'\n\n\n\nleft_df\n\nNameError: name 'left_df' is not defined\n\n\n\nright_df\n\nNameError: name 'right_df' is not defined\n\n\nThe column we want to perform the join on is the one containing information about the town. In the left DataFrame this has name Capital while in the right one Town.\n\ninner_join = pd.merge(left_df,right_df,left_on='Capital',right_on='Town',how='inner')\ninner_join\n\nNameError: name 'left_df' is not defined\n\n\nAs you may notice, the resulting DataFrame has only one line, the only row that the columns Capital and Town have in common (Amsterdam). This is because an inner join selects only those row values that are the same in the two columns (mathematically, an intersection). The columns of the two DataFrames are merged, even if they have the same name. In our case, both left and right DataFrames have a column with the same name (Elevation). After merging, the two columns are preserved, but with a suffix to distinguish them. If you are not happy with the default suffix, you may specify yours in the list of arguments of the pd.merge functions.\nLet’s now look at the other joins:\n\nleft_join = pd.merge(left_df,right_df,left_on='Capital',right_on='Town',how='left')\nleft_join\n\nNameError: name 'left_df' is not defined\n\n\n\nright_join = pd.merge(left_df,right_df,left_on='Capital',right_on='Town',how='right')\nright_join\n\nNameError: name 'left_df' is not defined\n\n\n\nouter_join = pd.merge(left_df,right_df,left_on='Capital',right_on='Town',how='outer')\nouter_join\n\nNameError: name 'left_df' is not defined\n\n\nTo resume: - An inner join selects rows that are in common to both left and right selected columns (intersection); - A left join selects rows that are in common to both left and right selected columns AND all the rows of the left DataFrame; - A right join selects rows that are in common to both left and right selected columns AND all the rows of the right DataFrame; - An outer join merges the two DataFrames.\nTo better understand how join works, it may be useful to look at the diagrams below:\n\n\n\n\n\nDo you want to select only common information between the two DataFrames? Then you would probably need an inner join;\n\n\nDo you want to add information to your left DataFrame? Then you would probably need a left join;\n\n\nDo you want to add information to your right DataFrame? Then you would probably need a right join;\n\n\nDo you want to get all the information from the two DataFrames? Then you would probably need an outer join."
  },
  {
    "objectID": "data-science-with-pandas-4.html#data-visualization-with-python",
    "href": "data-science-with-pandas-4.html#data-visualization-with-python",
    "title": "9  Data Science with Pandas (part 4: Data Visualization)",
    "section": "9.1 Data Visualization with python",
    "text": "9.1 Data Visualization with python\nMatplotlib is one of the most popular and widely-used data visualization libraries for Python. Matplotlib was inspired by the plotting functionalities of MATLAB (a non-open source programming language). It provides a comprehensive set of tools for creating a variety of plot types, such as line plots, scatter plots, bar plots, histograms, heatmaps, and many more.\nIn this session we will generate and plot some artificial data and data contained in our pandas DataFrames using pyplot. We will go through the main matplotlib concepts and we will generate several kinds of plots to illustrate matplotlib and pyplot potential."
  },
  {
    "objectID": "data-science-with-pandas-4.html#preliminaries",
    "href": "data-science-with-pandas-4.html#preliminaries",
    "title": "9  Data Science with Pandas (part 4: Data Visualization)",
    "section": "9.2 Preliminaries",
    "text": "9.2 Preliminaries\nWe begin importing the pandas package in the same way we did in previous sessions:\n\nimport pandas as pd\n\nThe first thing to do to start visualizing data with python is importing the module pyplot from the matplotlib library. As with many of our previous imports, we import the module under an ‘alias’ (alternate shorter name) for convenience. Finally, we specify the command %matplotlib inline so that, when plotting, Jupyter Notebook will not display the plots into new windows, but in the notebook itself.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "data-science-with-pandas-4.html#histograms",
    "href": "data-science-with-pandas-4.html#histograms",
    "title": "9  Data Science with Pandas (part 4: Data Visualization)",
    "section": "9.3 Histograms",
    "text": "9.3 Histograms\nIn the first example we will generate some artificial data by generating 10000 normally distributed values. In order to obtain that, we will first import the package numpy, a package used for scientific computing and data analysis. In particular, the sub-package numpy.random contains very handy tools to work with random numbers and its function .normal() generates normally distributed random numbers.\n\nimport numpy as np\nsample_data = np.random.normal(0, 0.1, 10000)\n\nIn this case we are drawing 10000 points with 0 average and a standard deviation of 0.1. First we will use the pyplot function hist() to visualize a histogram of our data:\n\nplt.hist(sample_data)\n\n(array([  22.,  153.,  683., 1807., 2787., 2626., 1368.,  458.,   87.,\n           9.]),\n array([-0.35645512, -0.28296465, -0.20947419, -0.13598372, -0.06249326,\n         0.01099721,  0.08448767,  0.15797814,  0.2314686 ,  0.30495907,\n         0.37844953]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\nAs we expected, the histogram is centered around 0 and we can already see the bell shape arising among the blocks. The default values of histogram bins (blocks) is 10, so in our case 10000 points are subdivided into 10 bins, but we can change that by specifying the parameter bins:\n\nplt.hist(sample_data, bins=30)\n\n(array([  3.,   9.,  10.,  26.,  44.,  83., 154., 208., 321., 458., 598.,\n        751., 865., 940., 982., 930., 896., 800., 594., 436., 338., 215.,\n        153.,  90.,  53.,  22.,  12.,   3.,   4.,   2.]),\n array([-0.35645512, -0.3319583 , -0.30746147, -0.28296465, -0.25846783,\n        -0.23397101, -0.20947419, -0.18497737, -0.16048054, -0.13598372,\n        -0.1114869 , -0.08699008, -0.06249326, -0.03799644, -0.01349961,\n         0.01099721,  0.03549403,  0.05999085,  0.08448767,  0.10898449,\n         0.13348132,  0.15797814,  0.18247496,  0.20697178,  0.2314686 ,\n         0.25596542,  0.28046225,  0.30495907,  0.32945589,  0.35395271,\n         0.37844953]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\n\n\n\nYou may have noticed that increasing the number of bins, the bell shape of the histogram is even more evident."
  },
  {
    "objectID": "data-science-with-pandas-4.html#customizing-plots",
    "href": "data-science-with-pandas-4.html#customizing-plots",
    "title": "9  Data Science with Pandas (part 4: Data Visualization)",
    "section": "9.4 Customizing plots",
    "text": "9.4 Customizing plots\nIn the previous examples we generated very simple plots to have a quick look at the data, either from an existing pandas DataFrame or working with artificial data. However, with Matplotlib you can customize many more aspects of your plot: axes, x and y ticks and labels, titles, legends, and much more.\nTo get full control of the plots generated with Matplotlib.pyplot, it is important to be aware of jargon used to describe different parts of the figures that you create.\n\nAt the higher level we have Figures. A Figure is simply the total white space where you will organise your plots. You may think of it as the white page were you are going to draw your plots or also as a box containing all your plots. You can both have a single plot per Figure or multiple plots sharing the same Figure\nAt a lower level we have Axes. Axes are contained into Figures. Axes is the name for a single plot or graphs (which may be a bit confusing since it can be mistaken for the x axis and y axis of a plot). You can have a single Axes per Figure, so one plot per Figure (see Plot1 on the left of the figure below) or multiple Axes per Figure, like in Plot2 (on the right) where the same Figure contains three plots distributed in two rows: two on top and one on the bottom\nFinally, each Axes (aka each plot) contains two Axis, i.e. x and y axis.\n\nTo summarize, matplotlib organizes plots into Figures, Axes, and Axis. A Figure is a canvas that can contain one or more Axes. An Axes is where data is plotted and it is made of two Axis, x and y. Specifying parameters at these three different levels, you can customize your plots to the finest details.\n\n\n\nPlot Hierarchy\n\n\nCertain attributes like the Figure size and the number of plots inside the Figure belong to the Figure level. Ticks, labels, plot title, legend, etc belong to the Axes level. Data is plotted on Axes according to the specified x and y Axis. The main features of a “typical” plot generated with matplotlib are well summarized by the picture below from matplotlib documentation:\n\n&lt;img src=\"images/anatomy.jpeg\" alt=\"Plot Main Features\" width=\"70%\" /&gt;"
  },
  {
    "objectID": "data-science-with-pandas-4.html#customizing-titles-and-labels",
    "href": "data-science-with-pandas-4.html#customizing-titles-and-labels",
    "title": "9  Data Science with Pandas (part 4: Data Visualization)",
    "section": "9.5 Customizing titles and labels",
    "text": "9.5 Customizing titles and labels\nTo add labels to the axis of a plot, we first create a figure containing 1 Axes (or one plot named ax). Then we use ax.set_xlabel('&lt;label&gt;') and ax.set_ylabel('&lt;label&gt;') to specify the axis labels. We can add a title to the figure using fig.suptitle('&lt;title&gt;')\n\nfig, ax = plt.subplots() # prepare a matplotlib figure\nax.hist(sample_data, bins=30)\n\n# add labels\n\nax.set_ylabel('density')\nax.set_xlabel('value')\nfig.suptitle('Histogram', fontsize=15)\n\nText(0.5, 0.98, 'Histogram')\n\n\n\n\n\nNote: To plot data on our Axes we used the same plotting methods used in the previous examples. We used hist() sampling the data in 30 bins, but this time we had to call the function from the Axes object, so ax.hist()."
  },
  {
    "objectID": "data-science-with-pandas-4.html#creating-subplots",
    "href": "data-science-with-pandas-4.html#creating-subplots",
    "title": "9  Data Science with Pandas (part 4: Data Visualization)",
    "section": "9.6 Creating subplots",
    "text": "9.6 Creating subplots\nIf we want to create a figure that consists of multiple subplots we can use the plt.subplots() functions. The first two arguments indicate the number of vertical and horizontal plots we want to fit in our Figure. In this case, we will create two plots side to side, so our grid will have one row and two columns. As we want to be sure that there will be enough space for our two plots, we specify the size of the Figure to be 12 inches long and 6 inches high (inches is the default size unit, but you can specify different ones).\nIn this case our settings produced plots distributed in one row and two columns, so a total of 2 plots, therefore plt.subplots() will return 2 Axes objects in a tuple. We will store these two Axes into the variables ax1 and ax2.\n\n# prepare a matplotlib figure\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(12,6))\nax1.hist(sample_data, bins=30)\n# add labels\nax1.set_ylabel('density')\nax1.set_xlabel('value')\n\n# define and sample beta distribution\na = 5\nb = 10\nbeta_draws = np.random.beta(a, b, 1000)\n\n# plot beta distribution\nax2.hist(beta_draws, bins=30)\nax2.set_ylabel('density')\nax2.set_xlabel('value')\n\nText(0.5, 0, 'value')\n\n\n\n\n\nInstead of plotting side by side, it is also possible to add a plot inside (or actually overlaying another plot).\nIn order to do this we can initiate at a Figure and an Axes (the plot to put inside it), using plt.subplots(), without specifying any argument (so without specifying the number or rows and columns), the method will return a Figure and a single Axes object. We will assign these two python objects to the variables fig and ax.\n\nfig, ax = plt.subplots()  # initiate an empty figure and axis matplotlib object\nax.hist(sample_data, bins= 30)\n\n(array([  3.,   9.,  10.,  26.,  44.,  83., 154., 208., 321., 458., 598.,\n        751., 865., 940., 982., 930., 896., 800., 594., 436., 338., 215.,\n        153.,  90.,  53.,  22.,  12.,   3.,   4.,   2.]),\n array([-0.35645512, -0.3319583 , -0.30746147, -0.28296465, -0.25846783,\n        -0.23397101, -0.20947419, -0.18497737, -0.16048054, -0.13598372,\n        -0.1114869 , -0.08699008, -0.06249326, -0.03799644, -0.01349961,\n         0.01099721,  0.03549403,  0.05999085,  0.08448767,  0.10898449,\n         0.13348132,  0.15797814,  0.18247496,  0.20697178,  0.2314686 ,\n         0.25596542,  0.28046225,  0.30495907,  0.32945589,  0.35395271,\n         0.37844953]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\n\n\n\nOnce we defined a Figure and an Axes, we can add other Axes to our Figure using fig.add_axes([left,bottom,length,height]) where the argument is a list containing the coordinates of our new Axes in the following format: [left edge, bottom edge, length, and height]. The left edge and bottom edgeare scaled from 0 to 1, so that 0.5 corresponds to the center of the Figure. For example, the list of coordinates [0.5,0.5,0.33,0.33] will locate the bottom-left corner of our additional Axis at the very center of the Figure. The new plot will be as wide as ~1/3 of the length of the Figure and as high as ~1/3 of the height of the Figure.\n\n# prepare a matplotlib figure\nfig, ax1 = plt.subplots()\nax1.hist(sample_data, 30)\n# add labels\nax1.set_ylabel('density')\nax1.set_xlabel('value')\n\n# define and sample beta distribution\na = 5\nb = 10\nbeta_draws = np.random.beta(a, b, 1000)\n\n# plot beta distribution\n# by adding additional axes to the figure\nax2 = fig.add_axes([0.65, 0.65, 0.2, 0.2])\n#ax2 = fig.add_axes([left, bottom, right, top])\nax2.hist(beta_draws, bins=30)\n\n(array([ 3.,  7.,  3., 22., 32., 39., 44., 46., 50., 66., 61., 61., 68.,\n        79., 66., 61., 60., 49., 48., 29., 27., 25., 19., 11.,  7.,  6.,\n         6.,  2.,  2.,  1.]),\n array([0.04860619, 0.07024462, 0.09188305, 0.11352148, 0.1351599 ,\n        0.15679833, 0.17843676, 0.20007518, 0.22171361, 0.24335204,\n        0.26499046, 0.28662889, 0.30826732, 0.32990574, 0.35154417,\n        0.3731826 , 0.39482103, 0.41645945, 0.43809788, 0.45973631,\n        0.48137473, 0.50301316, 0.52465159, 0.54629001, 0.56792844,\n        0.58956687, 0.61120529, 0.63284372, 0.65448215, 0.67612058,\n        0.697759  ]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\n\n\n\nplt.subplots() parameters allow you to specify all sort of plot features: the size of the Figure in inches or cm, the number of plots to display in the Figure arranged in rows and columns, whether the subplots need to share the same axis, etc. The Matplotlib documentation provides all the information and examples."
  },
  {
    "objectID": "data-science-with-pandas-4.html#plotting-grouped-data",
    "href": "data-science-with-pandas-4.html#plotting-grouped-data",
    "title": "9  Data Science with Pandas (part 4: Data Visualization)",
    "section": "9.7 Plotting grouped data",
    "text": "9.7 Plotting grouped data\nNow we will go back to the surveys dataset from the previous chapters:\n\nsurveys = pd.read_csv(('../course_materials/data/surveys.csv'))\n\n\nsurveys\n\n\n\n\n\n\n\n\nrecord_id\nmonth\nday\nyear\nplot_id\nspecies_id\nsex\nhindfoot_length\nweight\n\n\n\n\n0\n1\n7\n16\n1977\n2\nNL\nM\n32.0\nNaN\n\n\n1\n2\n7\n16\n1977\n3\nNL\nM\n33.0\nNaN\n\n\n2\n3\n7\n16\n1977\n2\nDM\nF\n37.0\nNaN\n\n\n3\n4\n7\n16\n1977\n7\nDM\nM\n36.0\nNaN\n\n\n4\n5\n7\n16\n1977\n3\nDM\nM\n35.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35544\n35545\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35545\n35546\n12\n31\n2002\n15\nAH\nNaN\nNaN\nNaN\n\n\n35546\n35547\n12\n31\n2002\n10\nRM\nF\n15.0\n14.0\n\n\n35547\n35548\n12\n31\n2002\n7\nDO\nM\n36.0\n51.0\n\n\n35548\n35549\n12\n31\n2002\n5\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n35549 rows × 9 columns\n\n\n\nNow first, let’s create a scatterplot where we plot the weight as a function of hindfoot_length:\n\nfig, ax1 = plt.subplots() # prepare a matplotlib figure\n\nsurveys.plot(\"hindfoot_length\", \"weight\", kind=\"scatter\", ax=ax1)\n\n# Provide further adaptations with matplotlib:\nax1.set_xlabel(\"Hindfoot length\")\nax1.tick_params(labelsize=16, pad=8)\nfig.suptitle('Scatter plot of weight versus hindfoot length', fontsize=15)\n\nText(0.5, 0.98, 'Scatter plot of weight versus hindfoot length')\n\n\n\n\n\n\nTRY IT YOURSELF: Time to play with plots! Look at the pandas.DataFrame.plot() documentation (here) and change your data visualization selecting different DataFrame columns, x and y axes, and kind of plot (try at least three different plots).\n\nIf we want to see if this distribution is different between males and females we can use a for loop and the groupby method to overlay two plots on top of each other in the same Axes object.\n\nfig, ax = plt.subplots()\nlabels = []\n\nfor i, group in list(surveys.groupby('sex')):\n    ax.scatter(group['hindfoot_length'], group['weight'], alpha=0.5)\n    labels.append(group['sex'].iloc[0])\n    \nax.legend(labels)\nax.set_xlabel(\"Hindfoot length\")\nax.set_ylabel(\"Weight\")\n\nText(0, 0.5, 'Weight')\n\n\n\n\n\nBy using ax.scatter 2 times inside the for loop, the two sets of points end up in the same Axes. Automatically the groups get different colors. With alpha=0.5 we can make the dots semi transparent so we can see them a bit better (however due to the large number of dots many blue dots are probably still hidden). We need to add the legend afterwards (after the for loop), but we do need the provide the labels in a list. We generated the list of labels in the for loop to make sure we don’t mix them up by accident.\nPerhaps we get a better view if we plot them in separate subplots, which is the next exercise:\n\n&lt;div class=\"alert alert-block alert-success\"&gt;\n&lt;b&gt;TRY IT YOURSELF&lt;/b&gt;: Plot DataFrame data in a single Figure:\n- Initialize a Figure with 2 Axes distributed in one rows and two columns;\n- In the first Axis plot the data for all Females, and in the second Axis, plot all Males.\n- Make your Axes (plots) \"pretty\": label all your axes, use clear character font, choose a nice title for your plot. You may want to consult the [`Axes.plot()` documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html).\n&lt;/div&gt;\n\nSyntaxError: invalid syntax (315948408.py, line 1)\n\n\n\n9.7.1 Saving your plot\nOnce you produced your plot you will probably need to share it in different media (website, papers, slide show, etc). To do that, we need to save our plot in a specific format. Once you have defined a Figure, you can do that with a single line of code:\n\nfig.savefig('MyFigure.png', dpi=200)\n\nThe Figure method savefig() will save your figure into a file. The first argument of the function is the name of the output file. Matplotlib will automatically recognize the format of the output file from its name. If you will specify only a name without extention, you can indicate the format explicitly with the parameter format. We also need to specify the dpi (dots per inch), i.e. the quality of the picture. The quality of the picture depends on the media we want to use. 200 dpi is good enough for visualization on a screen.\n\n\n9.7.2 What’s next?\nAs we mentioned in the introduction, matplotlib library is huge and you can customize every single little feature of your plot. With matplotlib you can also create animations and 3D plots. Now that you know the basics of plotting data, have a look at the matplotlib gallery to check the huge variety of plots you can generate with matplotlib.\nThis was the last chapter of this course! Go to What is next after this course, for tips on how to get started with Python in your own project!"
  },
  {
    "objectID": "what-next.html",
    "href": "what-next.html",
    "title": "What Next?",
    "section": "",
    "text": "See this website as reference: https://datacarpentry.org/python-ecology-lesson/00-before-we-start/index.html\nYou can continue on your programming journey using:\n\nhelp function\ndocumentation\nStackOverflow\nChatGPT haha\nWalk-In Hours\nProgramming Cafe"
  }
]